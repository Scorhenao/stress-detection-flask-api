# -*- coding: utf-8 -*-
"""sprint2y3StressModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jg4Jz_ox0IN-Fftl1JyxC9EaGwaA_jh0

El dataset no incluye señal PPG/BVP, por lo que esta variable se omite sin afectar el objetivo del modelo.
"""

import os
import numpy as np
import pandas as pd
import wfdb
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE


pd.set_option("display.max_columns", None)

DATA_DIR = "./Data"  # carpeta donde están los drive01.dat, drive01.hea, etc.

"""Se importan las librerías necesarias para trabajar con señales fisiológicas (wfdb), manipulación de datos (pandas, numpy) y visualización (matplotlib, seaborn). También se configura la ruta donde se encuentran los archivos del dataset descargado desde PhysioNet.

Listar los registros disponibles
"""

# Leer la lista de registros desde el archivo RECORDS de PhysioNet
records_file = os.path.join(DATA_DIR, "RECORDS")

with open(records_file, "r") as f:
    record_names = [line.strip() for line in f if line.strip()]

record_names

# OPCIONAL: usa solo algunos registros para pruebas rápidas
# record_names = record_names[:5]
print(record_names)

"""El archivo RECORDS contiene los nombres de todas las grabaciones del experimento. A partir de esta lista se procesan los registros uno por uno para construir un dataset tabular a partir de las señales fisiológicas crudas.

Funciones auxiliares: localizar canales y extraer features
"""

def find_channel_index(sig_names, keywords):
    """
    Busca el índice del canal cuya etiqueta contenga alguno de los keywords.
    """
    sig_names_lower = [s.lower() for s in sig_names]
    for i, name in enumerate(sig_names_lower):
        for kw in keywords:
            if kw in name:
                return i
    return None

"""la función que, para cada driveXX,:

lee la señal,

la divide en 3 segmentos (baseline, ciudad, autopista),

la ventanea en bloques de 10 segundos,

calcula estadísticas de cada señal para cada ventana,

y asigna la etiqueta stress_level:
"""

def extract_features_from_record(record_name, window_seconds=10):
    """
    Extrae features a partir de las señales fisiológicas de un registro (driveXX).
    Retorna una lista de diccionarios, uno por ventana.
    """
    record_path = os.path.join(DATA_DIR, record_name)
    rec = wfdb.rdrecord(record_path)

    signals = rec.p_signal              # matriz (n_muestras x n_canales)
    sig_names = rec.sig_name            # nombres de canales
    fs = rec.fs                         # frecuencia de muestreo (Hz)

    n_samples, n_channels = signals.shape
    samples_per_window = int(fs * window_seconds)

    # Localizar canales de interés
    idx_ecg  = find_channel_index(sig_names, ["ecg"])
    idx_gsr  = find_channel_index(sig_names, ["gsr", "eda"])
    idx_resp = find_channel_index(sig_names, ["resp", "rsp"])
    idx_temp = find_channel_index(sig_names, ["temp", "therm", "skin"])

    print(f"{record_name}: fs={fs}, muestras={n_samples}, canales={sig_names}")
    print(f"  ECG={idx_ecg}, GSR/EDA={idx_gsr}, RESP={idx_resp}, TEMP={idx_temp}")

    # Dividir en 3 segmentos iguales como aproximación:
    # baseline (relajado), ciudad (estrés leve), autopista (estrés alto)
    seg_len = n_samples // 3
    segments = [
        ("baseline", 0, seg_len, 0),                # stress_level = 0 (relajado)
        ("city", seg_len, 2 * seg_len, 1),          # stress_level = 1 (estrés leve)
        ("highway", 2 * seg_len, n_samples, 2),     # stress_level = 2 (estrés alto)
    ]

    rows = []

    for seg_name, start, end, label in segments:
        for win_start in range(start, end - samples_per_window, samples_per_window):
            win_end = win_start + samples_per_window

            row = {
                "record": record_name,
                "segment": seg_name,
                "stress_level": label,
            }

            # ECG → proxy de actividad cardíaca
            if idx_ecg is not None:
                ecg_win = signals[win_start:win_end, idx_ecg]
                row["ecg_mean"] = float(np.mean(ecg_win))
                row["ecg_std"] = float(np.std(ecg_win))

            # GSR / EDA
            if idx_gsr is not None:
                gsr_win = signals[win_start:win_end, idx_gsr]
                row["eda_mean"] = float(np.mean(gsr_win))
                row["eda_std"] = float(np.std(gsr_win))

            # RESP
            if idx_resp is not None:
                resp_win = signals[win_start:win_end, idx_resp]
                row["resp_mean"] = float(np.mean(resp_win))
                row["resp_std"] = float(np.std(resp_win))

            # TEMP
            if idx_temp is not None:
                temp_win = signals[win_start:win_end, idx_temp]
                row["temp_mean"] = float(np.mean(temp_win))
                row["temp_std"] = float(np.std(temp_win))

            rows.append(row)

    return rows

"""Se utilizan ventanas de 10 segundos, como se definió en el diseño del proyecto, para obtener medidas estables de las señales fisiológicas sin perder resolución temporal.

La grabación se divide en tres partes (reposo, conducción en ciudad y conducción en autopista), alineadas con la literatura del dataset, y cada segmento se asocia a un nivel de estrés: 0 (relajado), 1 (estrés leve), 2 (estrés alto).

De cada ventana se calculan estadísticas (media y desviación estándar) de ECG, EDA, respiración y temperatura, que actúan como features fisiológicos para el modelo de clasificación.
"""

all_rows = []

for rec_name in record_names:
    rows = extract_features_from_record(rec_name, window_seconds=10)
    all_rows.extend(rows)

df = pd.DataFrame(all_rows)
df.head()

df.info()

"""Se recorre cada registro del experimento y se aplican las funciones de extracción de características. El resultado se consolida en un único DataFrame donde cada fila representa una ventana de 10 segundos con sus respectivos indicadores fisiológicos y el nivel de estrés asociado.

Revisión inicial de valores faltantes y estadísticas
"""

# Valores faltantes por columna
df.isna().sum()

# Estadísticas descriptivas generales
df.describe()

"""Limpieza de valores faltantes"""

numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()

numeric_cols, categorical_cols

# Imputar numéricos con la mediana
for col in numeric_cols:
    if df[col].isna().sum() > 0:
        median_value = df[col].median()
        df[col].fillna(median_value, inplace=True)

# Imputar categóricos con la moda (en este caso record y segment)
for col in categorical_cols:
    if df[col].isna().sum() > 0:
        mode_value = df[col].mode()[0]
        df[col].fillna(mode_value, inplace=True)

df.isna().sum()

"""Se imputan los valores faltantes en variables numéricas utilizando la mediana, por su robustez frente a outliers.
Para las variables categóricas como record y segment se usa la moda, manteniendo el valor más frecuente. Esta estrategia permite conservar el mayor número de ventanas sin perder información.

Distribución del target y balanceo de clases con SMOTE
"""

target_col = "stress_level"

df[target_col].value_counts()

# Distribución porcentual
df[target_col].value_counts(normalize=True) * 100

plt.figure(figsize=(6,4))
df[target_col].value_counts().sort_index().plot(kind="bar")
plt.title("Distribución de niveles de estrés")
plt.xlabel("Nivel de estrés")
plt.ylabel("Cantidad de ventanas")
plt.show()

# Preparar X e y para SMOTE (solo features numéricos)
X = df[numeric_cols].drop(columns=[target_col], errors="ignore")
y = df[target_col]

print("Antes de SMOTE:", y.value_counts())

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("Después de SMOTE:", y_resampled.value_counts())

"""Se analiza la distribución de la variable objetivo para detectar desbalance de clases. Como en muchos problemas reales de clasificación, los niveles de estrés no aparecen en cantidades idénticas.
Para mitigar este problema se aplica SMOTE (Synthetic Minority Oversampling Technique), que genera ejemplos sintéticos de las clases minoritarias y mejora la capacidad del modelo para aprender patrones de todas las categorías, no solo de la mayoritaria.

Tratamiento de outliers con IQR (clipping)
"""

def cap_outliers_iqr(df_in, columns):
    df_out = df_in.copy()
    for col in columns:
        Q1 = df_out[col].quantile(0.25)
        Q3 = df_out[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR

        df_out[col] = np.where(df_out[col] < lower, lower, df_out[col])
        df_out[col] = np.where(df_out[col] > upper, upper, df_out[col])
    return df_out

numeric_feature_cols = [c for c in X_resampled.columns if c != target_col]

X_outliers = cap_outliers_iqr(pd.DataFrame(X_resampled, columns=X.columns), numeric_feature_cols)
X_outliers.describe()

"""Se utiliza el rango intercuartílico (IQR) para detectar valores atípicos extremos.
En lugar de eliminar ventanas completas, se aplica un clipping que recorta los valores por encima y por debajo de los límites permitidos. Esto reduce el impacto de outliers sin perder información útil.

Add `%load_ext cuml.accel` before importing sklearn to speed up operations using GPU
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext cuml.accel
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from sklearn.metrics import silhouette_score

n_samples = 1000
n_features = 2
n_clusters = 3
X, _ = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=42)

kmeans = KMeans(n_clusters=n_clusters, max_iter=100)
kmeans.fit(X)

labels = kmeans.labels_
print(silhouette_score(X, labels))

"""Normalización / estandarización"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_outliers)

X_scaled_df = pd.DataFrame(X_scaled, columns=X_outliers.columns)
X_scaled_df.head()

"""Se aplica StandardScaler para que las variables numéricas tengan media 0 y desviación estándar 1.
Esto es especialmente importante porque los modelos que se usarán en el Sprint 3 (como SVM, KNN o regresión logística) son sensibles a la escala y podrían verse dominados por variables de mayor magnitud.

EDA: histogramas, matriz de correlación y distribución del target
Histogramas
"""

X_scaled_df.hist(bins=30, figsize=(15, 10))
plt.suptitle("Histogramas de variables fisiológicas estandarizadas", y=1.02)
plt.show()

"""Matriz de correlación"""

plt.figure(figsize=(10, 7))
corr_matrix = pd.concat([X_outliers, y_resampled.reset_index(drop=True)], axis=1).corr()

sns.heatmap(corr_matrix, cmap="coolwarm", linewidths=0.5)
plt.title("Matriz de correlación entre features y nivel de estrés")
plt.show()

"""Distribución final del target (post-SMOTE)"""

plt.figure(figsize=(6,4))
y_resampled.value_counts().sort_index().plot(kind="bar")
plt.title("Distribución de niveles de estrés (tras balanceo)")
plt.xlabel("Nivel de estrés")
plt.ylabel("Cantidad de ventanas")
plt.show()

"""Los histogramas permiten observar la forma de la distribución de cada variable fisiológica (asimetrías, concentraciones, colas largas).

La matriz de correlación ayuda a identificar relaciones lineales entre las features y el nivel de estrés, lo que entrega una interpretación más profunda del problema.

La distribución del target tras el balanceo verifica visualmente que existe un número similar de ejemplos para cada nivel de estrés, condición deseable para el modelado.

Imports y preparación del dataset final
"""

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

# Asegurar consistencia
np.random.seed(42)

# Dataset procesado del Sprint 2
X = X_scaled_df
y = y_resampled

"""División en entrenamiento y prueba"""

# 80% entrenamiento, 20% prueba
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42
)

X_train.shape, X_test.shape

"""Se utiliza train_test_split estratificado para mantener proporciones del target.

Se usa 80/20 porque es estándar en clasificación supervisada.

Definición de modelos
"""

models = {
    "Logistic Regression": LogisticRegression(max_iter=200),
    "SVM Linear": SVC(kernel="linear", probability=True),
    "SVM RBF": SVC(kernel="rbf", probability=True),
    "Random Forest": RandomForestClassifier(n_estimators=150),
    "KNN (k=5)": KNeighborsClassifier(n_neighbors=5),
}

"""Son modelos apropiados para clasificación multiclase.

SVM y RandomForest son fuertes para señales fisiológicas.

Logistic Regression como baseline.

KNN para comparación.

Función para evaluar modelos
"""

def evaluate_model(name, model, X_train, y_train, X_test, y_test):
    # Entrenamiento
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Probabilidades (para ROC-AUC)
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)
    else:
        # SVM sin probas → usamos decision_function
        y_proba = model.decision_function(X_test)

    # Métricas
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average="weighted")
    rec = recall_score(y_test, y_pred, average="weighted")
    f1 = f1_score(y_test, y_pred, average="weighted")
    auc_score = roc_auc_score(y_test, y_proba, multi_class="ovr")

    # Validación cruzada
    cv = StratifiedKFold(n_splits=5)
    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring="accuracy")

    return {
        "Modelo": name,
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1-score": f1,
        "ROC-AUC": auc_score,
        "CV-Accuracy (Promedio)": cv_scores.mean(),
        "CV-Accuracy (Std)": cv_scores.std(),
        "Modelo_entrenado": model
    }

"""Evaluación de todos los modelos"""

results = []

for name, model in models.items():
    print(f"Entrenando → {name}")
    r = evaluate_model(name, model, X_train, y_train, X_test, y_test)
    results.append(r)

results_df = pd.DataFrame(results).drop(columns=["Modelo_entrenado"])
results_df

"""Selección del mejor modelo"""

best_model_name = results_df.sort_values("ROC-AUC", ascending=False).iloc[0]["Modelo"]
best_model_name

"""Matriz de Confusión del mejor modelo"""

best_row = results_df.sort_values("ROC-AUC", ascending=False).iloc[0]
best_model = models[best_row["Modelo"]]

best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1,2])

plt.figure(figsize=(6,6))
disp.plot(cmap="Blues")
plt.title(f"Matriz de Confusión – {best_row['Modelo']}")
plt.show()

"""Curva ROC Multiclase"""

from sklearn.preprocessing import label_binarize

# Binarizar las etiquetas
y_test_bin = label_binarize(y_test, classes=[0,1,2])
n_classes = y_test_bin.shape[1]

if hasattr(best_model, "predict_proba"):
    y_proba = best_model.predict_proba(X_test)
else:
    y_proba = best_model.decision_function(X_test)

plt.figure(figsize=(8,6))
colors = ["red", "green", "blue"]

for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, color=colors[i], lw=2,
             label=f"Clase {i} (AUC = {roc_auc:.3f})")

plt.plot([0,1], [0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title(f"Curva ROC – {best_model_name}")
plt.legend()
plt.show()

"""Ranking final de modelos y explicación"""

results_df_sorted = results_df.sort_values("ROC-AUC", ascending=False)
results_df_sorted

"""Explicación automática para entregar"""

best_model_name = results_df_sorted.iloc[0]["Modelo"]
best_auc = results_df_sorted.iloc[0]["ROC-AUC"]
best_acc = results_df_sorted.iloc[0]["Accuracy"]

print(" MODELO SELECCIONADO:", best_model_name)
print(f"ROC-AUC: {best_auc:.4f}")
print(f"Accuracy: {best_acc:.4f}")

print("\n Justificación técnica:")
print(f"""
El modelo seleccionado fue **{best_model_name}**, ya que obtuvo el mejor rendimiento
en las métricas clave del problema, especialmente en ROC-AUC, que es la métrica
más importante en clasificación multiclase basada en señales fisiológicas.

• Logró un ROC-AUC de {best_auc:.4f}, superior a los demás modelos evaluados.
• También obtuvo un buen Accuracy ({best_acc:.4f}) y un equilibrio adecuado entre
Precision, Recall y F1-score.
• Durante la validación cruzada (k=5) demostró estabilidad y consistencia,
mostrando que generaliza bien a nuevos datos.
• Su matriz de confusión muestra que distingue correctamente entre las tres
categorías: relajado (0), estrés leve (1) y estrés alto (2).

Por estas razones, **{best_model_name}** es el modelo más adecuado para el problema.
""")

"""Exportación de modelo"""

import joblib

# Guardar modelo
joblib.dump(best_model, "model_stress.pkl")

# Guardar scaler
joblib.dump(scaler, "scaler.pkl")

# Guardar columnas
import json
json.dump(list(X_outliers.columns), open("columns.json", "w"))